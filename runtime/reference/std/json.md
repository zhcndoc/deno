---
title: "@std/json"
description: "（流式）解析和序列化 JSON 文件"
jsr: jsr:@std/json
pkg: json
version: 1.0.2
generated: true
stability: stable
---

<!-- Autogenerated from JSR docs. Do not edit directly. -->

## 概览

<p>用于解析流式 JSON 数据的工具。</p>

```js
import { JsonStringifyStream } from "@std/json";
import { assertEquals } from "@std/assert";

const stream = ReadableStream.from([{ foo: "bar" }, { baz: 100 }])
  .pipeThrough(new JsonStringifyStream());

assertEquals(await Array.fromAsync(stream), [
  `{"foo":"bar"}\n`,
  `{"baz":100}\n`,
]);
```

### 添加到你的项目

```sh
deno add jsr:@std/json
```

<a href="https://jsr.io/@std/json/doc" class="docs-cta jsr-cta">查看 @std/json 中的所有符号
<svg class="inline ml-1" viewBox="0 0 13 7" aria-hidden="true" height="20"><path d="M0,2h2v-2h7v1h4v4h-2v2h-7v-1h-4" fill="#083344"></path><g fill="#f7df1e"><path d="M1,3h1v1h1v-3h1v4h-3"></path><path d="M5,1h3v1h-2v1h2v3h-3v-1h2v-1h-2"></path><path d="M9,2h3v2h-1v-1h-1v3h-1"></path></g></svg></a>

<!-- custom:start -->

## 什么是 JSON 流？

JSON 流是一种处理 JSON 数据的连续流技术，而非一次性将整个数据集加载到内存中。这对于处理大型 JSON 文件或实时数据流尤其有用。

## 为什么使用 @std/json？

用于流式读写 JSON，处理大型数据集时无需将所有数据加载到内存中。

## 示例

解析连续拼接的 JSON（多个 JSON 值紧密连接）

```ts
import {
  ConcatenatedJsonParseStream,
} from "@std/json/concatenated-json-parse-stream";

// 流中包含两个不带分隔符的连续 JSON 文档。
const input = ReadableStream.from([
  '{"a":1}{',
  '"b":2}',
]);

const parsed = input.pipeThrough(new ConcatenatedJsonParseStream());
console.log(await Array.fromAsync(parsed)); // [{ a: 1 }, { b: 2 }]
```

从对象生成 NDJSON（JSON 行格式）

```ts
import { JsonStringifyStream } from "@std/json/stringify-stream";

const data = [{ id: 1 }, { id: 2 }, { id: 3 }];

// 在每个 JSON 值后添加换行符，形成 NDJSON
const ndjson = ReadableStream
  .from(data)
  .pipeThrough(new JsonStringifyStream({ suffix: "\n" }));

// 发送到接受 application/x-ndjson 的服务器
await fetch("/ingest", {
  method: "POST",
  headers: { "content-type": "application/x-ndjson" },
  body: ndjson,
});
```

安全地消费 NDJSON（处理跨块边界的换行）

```ts
import { JsonParseStream } from "@std/json/parse-stream";

// 按换行符拆分，即使一行跨越多个块
function lineSplitter() {
  let buffer = "";
  return new TransformStream<string, string>({
    transform(chunk, controller) {
      buffer += chunk;
      const lines = buffer.split(/\r?\n/);
      buffer = lines.pop() ?? ""; // 保留最后不完整的行
      for (const line of lines) if (line) controller.enqueue(line);
    },
    flush(controller) {
      if (buffer) controller.enqueue(buffer);
    },
  });
}

const res = await fetch("/stream.ndjson");
const values = res.body!
  .pipeThrough(new TextDecoderStream())
  .pipeThrough(lineSplitter())
  .pipeThrough(new JsonParseStream());

for await (const obj of values) {
  // 按到达顺序处理每个 JSON 对象
  console.log(obj);
}
```

动态转换 JSON 流

```ts
import { JsonParseStream } from "@std/json/parse-stream";
import { JsonStringifyStream } from "@std/json/stringify-stream";

// 输入对象 -> 映射 -> 输出 NDJSON
function mapStream<T, U>(map: (t: T) => U) {
  return new TransformStream<T, U>({ transform: (t, c) => c.enqueue(map(t)) });
}

const response = await fetch("/objects.jsonl");
const uppercased = response.body!
  .pipeThrough(new TextDecoderStream())
  .pipeThrough(lineSplitter()) // 来自前面示例
  .pipeThrough(new JsonParseStream<{ name: string }>())
  .pipeThrough(mapStream((o) => ({ name: o.name.toUpperCase() })))
  .pipeThrough(new JsonStringifyStream({ suffix: "\n" }));

// 管道输出到另一个请求、文件或进一步处理
await fetch("/store", { method: "POST", body: uppercased });
```

## 提示

- 使用 `JsonStringifyStream` / `JsonParseStream` 可与 `fetch()` 和文件流高效组合。
- 明确编码边界 —— 使用 UTF-8 和 `TextEncoder` / `TextDecoder` 作为桥接。
- 生成 NDJSON 时使用 `JsonStringifyStream({ suffix: "\n" })`，消费时先拆分行再用 `JsonParseStream`。
- 输入为紧凑连续的 JSON 值流时，使用 `ConcatenatedJsonParseStream`。

<!-- custom:end -->